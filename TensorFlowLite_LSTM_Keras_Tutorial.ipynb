{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRWOI1nxutyx"
   },
   "source": [
    "# Overview\n",
    "This codelab will demonstrate how to build a LSTM model for MNIST recognition using keras & how to convert the model to TensorFlow Lite.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXzpJuM7zujk"
   },
   "outputs": [],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOE_xIJuvMOU"
   },
   "source": [
    "### Prerequisites\n",
    "We're going to override the environment variable `TF_ENABLE_CONTROL_FLOW_V2` since for TensorFlow Lite control flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is important!\n",
    "import os\n",
    "os.environ['TF_ENABLE_CONTROL_FLOW_V2'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Bidirectional, BatchNormalization, SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/bsuthar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in DataSet: 1193514\n"
     ]
    }
   ],
   "source": [
    "# GLOVE--EMBEDDING\n",
    "def read_data(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        word_vocab = set() # not using list to avoid duplicate entry\n",
    "        word2vector = {}\n",
    "        for line in f:\n",
    "            line_ = line.strip() #Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word2vector[words_Vec[0]] = numpy.array(words_Vec[1:],dtype=float)\n",
    "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
    "    return word_vocab,word2vector\n",
    "\n",
    "word_vocab,w2v = read_data('Glove/glove.twitter.27B.25d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## While preprocessing total word count in corpus is stored\n",
    "word_count = []\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text.lower())\n",
    "\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z ]+', '', text)\n",
    "\n",
    "    # remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "    word_net_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = [word_net_lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    # ps = nltk.stem.PorterStemmer()\n",
    "    #\n",
    "    # text = [ps.stem(word) for word in text]\n",
    "\n",
    "    #word_count.append(len(text))\n",
    "\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m gallery</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in gallatin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m garrity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>am gallatin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an reality</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in reality</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and garity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>am galtee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adam reality</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>and galatea</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my event</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all lament</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my element</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my ornament</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all limit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aluminium it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a lumen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>an idiot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a aluminate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a lumiere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>truly city</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the city</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truly sweetie</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>truly sweety</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to the city</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>julie city</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>holy city</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rowley city</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>to list</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>luxury city</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  feature\n",
       "0      m gallery        0\n",
       "1    in gallatin        0\n",
       "2      m garrity        0\n",
       "3    am gallatin        0\n",
       "4     an reality        0\n",
       "5     in reality        0\n",
       "6     and garity        0\n",
       "7      am galtee        0\n",
       "8   adam reality        0\n",
       "9    and galatea        0\n",
       "0       my event        1\n",
       "1     all lament        1\n",
       "2     my element        1\n",
       "3    my ornament        1\n",
       "4      all limit        1\n",
       "5   aluminium it        1\n",
       "6        a lumen        1\n",
       "7       an idiot        1\n",
       "8    a aluminate        1\n",
       "9      a lumiere        1\n",
       "0     truly city        2\n",
       "1       the city        2\n",
       "2  truly sweetie        2\n",
       "3   truly sweety        2\n",
       "4    to the city        2\n",
       "5     julie city        2\n",
       "6      holy city        2\n",
       "7    rowley city        2\n",
       "8        to list        2\n",
       "9    luxury city        2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Dset/drug_names.xlsx')\n",
    "\n",
    "emgality = df['emgality'].values\n",
    "olumiant = df['olumiant'].values\n",
    "trulicity = df['trulicity'].values\n",
    "\n",
    "\n",
    "appeneded = emgality + olumiant + trulicity\n",
    "feature_emgality = []\n",
    "feature_olumiant = []\n",
    "feature_trulicity = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(emgality)):\n",
    "    feature_emgality.append(0)\n",
    "for i in range(len(olumiant)):\n",
    "    feature_olumiant.append(1)\n",
    "for i in range(len(trulicity)):\n",
    "    feature_trulicity.append(2)\n",
    "\n",
    "    \n",
    "data_emgality = pd.DataFrame({'text':emgality, 'feature':feature_emgality})\n",
    "data_olumiant = pd.DataFrame({'text':olumiant, 'feature':feature_olumiant} )\n",
    "data_trulicity = pd.DataFrame({'text':trulicity, 'feature':feature_trulicity} )\n",
    "\n",
    "\n",
    "data_frame = pd.concat([data_emgality,data_olumiant,data_trulicity])\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.text = data_frame.text.apply(preprocess)\n",
    "\n",
    "X = data_frame.text\n",
    "y = data_frame.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 25)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = numpy.array(word_count)\n",
    "max_length = 10\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_frame.text)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=max_length, padding='post')\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = numpy.zeros((num_words, 25))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 city\n",
      "2 reality\n",
      "3 truly\n",
      "4 gallatin\n",
      "5 gallery\n",
      "6 garrity\n",
      "dummy matrix\n",
      "dummy matrix\n",
      "9 adam\n",
      "10 galatea\n",
      "11 event\n",
      "12 lament\n",
      "13 element\n",
      "14 ornament\n",
      "15 limit\n",
      "16 aluminium\n",
      "17 lumen\n",
      "18 idiot\n",
      "dummy matrix\n",
      "20 lumiere\n",
      "21 sweetie\n",
      "22 sweety\n",
      "23 julie\n",
      "24 holy\n",
      "25 rowley\n",
      "26 list\n",
      "27 luxury\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 25)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_matrix = numpy.zeros(shape = (25,))\n",
    "\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    try:\n",
    "\n",
    "        embedding_vector = w2v[word]\n",
    "        if embedding_vector is not None:\n",
    "            print(i,word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        print('dummy matrix')\n",
    "        #embedding_matrix[i] = dummy_matrix\n",
    "        \n",
    "    \n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3Ku1Lx9vvfX"
   },
   "source": [
    "## Step 1 Build the MNIST LSTM model.\n",
    "\n",
    "Note we will be using **`tf.lite.experimental.nn.TFLiteLSTMCell`** & **`tf.lite.experimental.nn.dynamic_rnn`** in the tutorial.\n",
    "\n",
    "Also note here, we're not trying to build the model to be a real world application, but only demonstrates how to use TensorFlow lite. You can a build a much better model using CNN models.\n",
    "\n",
    "For more canonical lstm codelab, please see [here](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiYZoDlC5SEJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 25)            700       \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 64)                56064     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 57,024\n",
      "Trainable params: 57,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Build the MNIST LSTM model.\n",
    "def buildLstmLayer(inputs, num_layers, num_units):\n",
    "  \"\"\"Build the lstm layer.\n",
    "\n",
    "  Args:\n",
    "    inputs: The input data.\n",
    "    num_layers: How many LSTM layers do we want.\n",
    "    num_units: The unmber of hidden units in the LSTM cell.\n",
    "  \"\"\"\n",
    "  lstm_cells = []\n",
    "  for i in range(num_layers):\n",
    "    lstm_cells.append(\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, forget_bias=0, name='rnn{}'.format(i)))\n",
    "  lstm_layers = tf.keras.layers.StackedRNNCells(lstm_cells)\n",
    "  # Assume the input is sized as [batch, time, input_size], then we're going\n",
    "  # to transpose to be time-majored.\n",
    "  transposed_inputs = tf.transpose(\n",
    "      inputs, perm=[1, 0, 2])\n",
    "  outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\n",
    "      lstm_layers,\n",
    "      transposed_inputs,\n",
    "      dtype='float32',\n",
    "      time_major=True)\n",
    "  unstacked_outputs = tf.unstack(outputs, axis=0)\n",
    "  return unstacked_outputs[-1]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=(X.shape[1],), name='input'),\n",
    "  tf.keras.layers.Embedding(num_words , 25,weights=[embedding_matrix],trainable=True,input_length=X.shape[1]),\n",
    "  tf.keras.layers.Lambda(buildLstmLayer, arguments={'num_layers' : 2, 'num_units' : 64}),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(4, activation=tf.nn.softmax, name='output')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ff6X9gg_wk7K"
   },
   "source": [
    "## Step 2: Train & Evaluate the model.\n",
    "We will train the model using MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23W41fiRPOmh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 0s 716us/sample - loss: 0.3540 - acc: 0.8333 - val_loss: 8.8891e-04 - val_acc: 1.0000\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 641us/sample - loss: 0.1698 - acc: 0.9667 - val_loss: 9.1544e-04 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 616us/sample - loss: 0.1557 - acc: 0.9667 - val_loss: 9.3533e-04 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 679us/sample - loss: 0.1840 - acc: 0.9000 - val_loss: 9.9561e-04 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 612us/sample - loss: 0.1126 - acc: 0.9667 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 624us/sample - loss: 0.0734 - acc: 0.9667 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 634us/sample - loss: 0.0448 - acc: 0.9667 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 619us/sample - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 667us/sample - loss: 0.0103 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 643us/sample - loss: 0.0102 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 640us/sample - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 712us/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 681us/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 747us/sample - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 715us/sample - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 0s 640us/sample - loss: 0.0088 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 0s 658us/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 0s 609us/sample - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 0s 616us/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 0s 652us/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 0s 663us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 0s 629us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 0s 621us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 0s 641us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 9.8606e-04 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 0s 656us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 9.2647e-04 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 611us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 8.8221e-04 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 0s 663us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 8.4795e-04 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 0s 673us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 8.2066e-04 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 643us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 7.9827e-04 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 0s 629us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 7.7956e-04 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 0s 608us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 7.6356e-04 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 0s 614us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 7.4973e-04 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 0s 643us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 7.3763e-04 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 0s 580us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 7.2689e-04 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 0s 591us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 7.1732e-04 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 0s 590us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 7.0864e-04 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 0s 577us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 7.0076e-04 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 0s 542us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 6.9353e-04 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 0s 581us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 6.8693e-04 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 0s 586us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 6.8078e-04 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 0s 601us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 6.7513e-04 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 0s 612us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 6.6978e-04 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 0s 603us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 6.6481e-04 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 0s 581us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 6.6015e-04 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 0s 631us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 6.5572e-04 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 650us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 6.5146e-04 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 0s 611us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 6.4748e-04 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 0s 602us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 6.4364e-04 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 606us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 6.3990e-04 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 651us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 6.3642e-04 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 620us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 6.3298e-04 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 596us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 6.2966e-04 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 0s 559us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 6.2643e-04 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 0s 583us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 6.2336e-04 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 570us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 6.2028e-04 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 529us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 6.1727e-04 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 555us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 6.1440e-04 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 601us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 6.1154e-04 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 577us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 6.0871e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "30/30 [==============================] - 0s 643us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 6.0596e-04 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 591us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 6.0318e-04 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 564us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 6.0051e-04 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 636us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 5.9787e-04 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 0s 581us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 5.9525e-04 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 552us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 5.9259e-04 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 0s 579us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.9004e-04 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 575us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.8745e-04 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 589us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.8488e-04 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 0s 687us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.8241e-04 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 0s 606us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.7989e-04 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 624us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.7737e-04 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 624us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.7490e-04 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 0s 601us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.7244e-04 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 0s 610us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.6999e-04 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 0s 647us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.6752e-04 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 625us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 5.6509e-04 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 584us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 5.6266e-04 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 578us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 5.6022e-04 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 594us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 5.5781e-04 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 0s 586us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 5.5544e-04 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 0s 596us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 5.5304e-04 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 572us/sample - loss: 9.9947e-04 - acc: 1.0000 - val_loss: 5.5066e-04 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 558us/sample - loss: 9.9144e-04 - acc: 1.0000 - val_loss: 5.4828e-04 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 555us/sample - loss: 9.8351e-04 - acc: 1.0000 - val_loss: 5.4591e-04 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 0s 548us/sample - loss: 9.7571e-04 - acc: 1.0000 - val_loss: 5.4356e-04 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 0s 563us/sample - loss: 9.6805e-04 - acc: 1.0000 - val_loss: 5.4119e-04 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 0s 560us/sample - loss: 9.6052e-04 - acc: 1.0000 - val_loss: 5.3885e-04 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 0s 523us/sample - loss: 9.5311e-04 - acc: 1.0000 - val_loss: 5.3651e-04 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 0s 534us/sample - loss: 9.4576e-04 - acc: 1.0000 - val_loss: 5.3421e-04 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 0s 541us/sample - loss: 9.3858e-04 - acc: 1.0000 - val_loss: 5.3190e-04 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 0s 545us/sample - loss: 9.3148e-04 - acc: 1.0000 - val_loss: 5.2960e-04 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 0s 530us/sample - loss: 9.2450e-04 - acc: 1.0000 - val_loss: 5.2727e-04 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 0s 526us/sample - loss: 9.1761e-04 - acc: 1.0000 - val_loss: 5.2494e-04 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 0s 517us/sample - loss: 9.1082e-04 - acc: 1.0000 - val_loss: 5.2270e-04 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 0s 541us/sample - loss: 9.0415e-04 - acc: 1.0000 - val_loss: 5.2043e-04 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 0s 531us/sample - loss: 8.9758e-04 - acc: 1.0000 - val_loss: 5.1814e-04 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 0s 545us/sample - loss: 8.9108e-04 - acc: 1.0000 - val_loss: 5.1589e-04 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 0s 532us/sample - loss: 8.8467e-04 - acc: 1.0000 - val_loss: 5.1366e-04 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 0s 536us/sample - loss: 8.7836e-04 - acc: 1.0000 - val_loss: 5.1136e-04 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 0s 554us/sample - loss: 8.7213e-04 - acc: 1.0000 - val_loss: 5.0914e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x152169b10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Train & Evaluate the model.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# # Cast x_train & x_test to float32.\n",
    "# x_train = x_train.astype(np.float32)\n",
    "# x_test = x_test.astype(np.float32)\n",
    "\n",
    "model.fit(X, y, epochs=100,validation_data=(x_test,y_test))\n",
    "# model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22, 10), (22,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'truly city'\n",
    "data = preprocess(data)\n",
    "data = tokenizer.texts_to_sequences([data])\n",
    "data = pad_sequences(data, maxlen=max_length, padding='post')\n",
    "results = model.predict([data])\n",
    "np.argmax(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtPJGiIQw0nM"
   },
   "source": [
    "## Step 3: Convert the Keras model to TensorFlow Lite model.\n",
    "\n",
    "Note here: we just convert to TensorFlow Lite model as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     1\n",
       "38    3\n",
       "12    0\n",
       "35    0\n",
       "0     1\n",
       "     ..\n",
       "21    1\n",
       "31    3\n",
       "27    1\n",
       "6     3\n",
       "11    0\n",
       "Name: feature, Length: 117, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 6\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tbuu_8PFz-x_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Convert the Keras model to TensorFlow Lite model.\n",
    "sess = tf.keras.backend.get_session()\n",
    "input_tensor = sess.graph.get_tensor_by_name('input:0')\n",
    "output_tensor = sess.graph.get_tensor_by_name('output/Softmax:0')\n",
    "converter = tf.lite.TFLiteConverter.from_session(\n",
    "    sess, [input_tensor], [output_tensor])\n",
    "tflite = converter.convert()\n",
    "open(\"auto_correction.tflite\",\"wb\").write(tflite)\n",
    "print('Model converted successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rHrZkIuxxar"
   },
   "source": [
    "## Step 4: Check the converted TensorFlow Lite model.\n",
    "\n",
    "We're just going to load the TensorFlow Lite model and use the TensorFlow Lite python interpreter to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lao097MnFf2"
   },
   "outputs": [],
   "source": [
    "# Step 4: Check the converted TensorFlow Lite model.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite)\n",
    "\n",
    "try:\n",
    "  interpreter.allocate_tensors()\n",
    "except ValueError:\n",
    "  assert False\n",
    "\n",
    "MINI_BATCH_SIZE = 1\n",
    "correct_case = 0\n",
    "for i in range(len(x_test)):\n",
    "  input_index = (interpreter.get_input_details()[0]['index'])\n",
    "  interpreter.set_tensor(input_index, x_test[i * MINI_BATCH_SIZE: (i + 1) * MINI_BATCH_SIZE])\n",
    "  interpreter.invoke()\n",
    "  output_index = (interpreter.get_output_details()[0]['index'])\n",
    "  result = interpreter.get_tensor(output_index)\n",
    "  # Reset all variables so it will not pollute other inferences.\n",
    "  interpreter.reset_all_variables()\n",
    "  # Evaluate.\n",
    "  prediction = np.argmax(result)\n",
    "  if prediction == y_test[i]:\n",
    "    correct_case += 1\n",
    "\n",
    "print('TensorFlow Lite Evaluation result is {}'.format(correct_case * 1.0 / len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e39cec71cc0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'json'"
     ]
    }
   ],
   "source": [
    "tokenizer.json.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlowLite_LSTM_Keras_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
